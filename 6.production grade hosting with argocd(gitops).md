# **creating an EC2 instance in AWS** and **accessing it from your local terminal using SSH** with a `.pem` file.


## **Step 1: Create an EC2 Instance**

1. **Login to AWS Console**
   Go to [AWS EC2 Console](https://console.aws.amazon.com/ec2/).

2. **Launch Instance**

   * Click **Launch instances**.
   * **Name:** Give your instance a name.
   * **AMI (Amazon Machine Image):** Choose a Linux AMI (e.g., Ubuntu).
   * **Instance Type:** Choose according to your needs (e.g., `t2.micro` for free tier or `c7i-flex.large`).
   * **Key Pair:**

     * Create a new key pair if you donâ€™t have one.
     * Download the **`.pem` file**. Keep it safe!

3. **Add Storage**

   * Default 8â€“20 GB is usually fine.

4. **Add Tags** (Optional)

   * Example: Key=`Name`, Value=`MyEC2Instance`.

5. **Configure Security Group**

   * Allow **SSH (Port 22)** from your IP address (`My IP`).
   * Optionally, allow HTTP/HTTPS if you are running web apps.

6. **Launch Instance**

   * Review and **Launch**.
   * Select your **existing key pair** or the one you just created.

---

## **Step 2: Set Permissions for `.pem` File**

On your local machine, open terminal:

```bash
chmod 400 /path/to/your-key-file.pem
```

* This ensures your key is **read-only**, which SSH requires.

---

## **Step 3: SSH Into EC2 Instance**

1. Find **Public IPv4** of your instance (e.g., `54.123.45.67`).

2. Use this command:

```bash
ssh -i /path/to/your-key-file.pem ec2-user@PUBLIC_IP
```

* Replace `/path/to/your-key-file.pem` with the actual path of your `.pem` file.
* Replace `PUBLIC_IP` with the instanceâ€™s public IP.
* `ec2-user` is default username for Amazon Linux. For Ubuntu AMIs, use `ubuntu` instead.

**Example:**

```bash
ssh -i ~/Downloads/my-key.pem ec2-user@54.123.45.67
```
**Example:**

```bash
ssh -i "aws-key-pair.pem" ubuntu@ec2-56-228-32-62.eu-north-1.compute.amazonaws.com (copy past this in where .pem file)
```

---

## **Step 4: Verify Connection**

If successful, youâ€™ll see something like:

```text
[Ubuntu@ip-172-31-44-153 ~]$
```

Now you are **connected to your EC2 instance** and can run commands remotely.

---

âœ… **Tips:**

* If SSH fails:

  * Check **security group** allows your IP.
  * Check **public IP** is assigned.
  * Ensure **`.pem` file permissions** are `400`.


<br>
<br>


---
---
---

<br>
<br>


# AWS CLI & EKS Setup Guide

This document describes the process of setting up **AWS CLI v2**, configuring **IAM user access**, installing **kubectl** and **eksctl**, and creating an **EKS cluster**.

---

## 1. Update and Upgrade Ubuntu

```bash
sudo apt update && sudo apt upgrade -y
````

---

## 2. Install AWS CLI v2

```bash
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
sudo apt install unzip -y
unzip awscliv2.zip
sudo ./aws/install -i /usr/local/aws-cli -b /usr/local/bin --update
```

Verify installation:

```bash
aws --version
# Example output: aws-cli/2.31.10 Python/3.13.7 Linux/6.14.0-1011-aws exe/x86_64.ubuntu.24
```

---

## 3. Configure AWS CLI

```bash
aws configure
```

Enter:

* **AWS Access Key ID:** `<Your Access Key ID>`
* **AWS Secret Access Key:** `<Your Secret Key>`
* **Default region name:** `eu-north-1` (or your preferred region)
* **Default output format:** `json` (optional)

Verify access:

```bash
aws s3 ls
aws sts get-caller-identity
```

Sample output:

```json
{
    "UserId": "AIDAYJRLT5B5ZOUVTEVJD",
    "Account": "570248390779",
    "Arn": "arn:aws:iam::570248390779:user/eks-admin"
}
```

---

## 4. Install kubectl

```bash
curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.19.6/2021-01-05/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin
kubectl version --short --client
```

Sample output:

```
Client Version: v1.19.6-eks-49a6c0
```

---

## 5. Install eksctl

```bash
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
eksctl version
```

Sample output:

```
0.215.0
```

---

## 6. Create EKS Cluster

```bash
eksctl create cluster \
  --name three-tier-cluster \
  --region ap-south-1 \
  --node-type c7i-flex.large \
  --nodes 2 \
  --nodes-min 2 \
  --nodes-max 3 \
  --managed
```

---

## 7. Update kubeconfig

```bash
aws eks update-kubeconfig --region ap-south-1 --name three-tier-cluster # if necessary
```

---

## 8. Verify Cluster Nodes

```bash
kubectl get nodes
```

Sample output:

```
NAME                                           STATUS   ROLES    AGE   VERSION
ip-192-168-57-32.ap-south-1.compute.internal   Ready    <none>   13m   v1.32.9-eks-113cf36
ip-192-168-87-68.ap-south-1.compute.internal   Ready    <none>   13m   v1.32.9-eks-113cf36
```

---

### Notes

* IAM user should have administrative access **only for dev/test environments**, not production-grade.
* Always keep your `.pem` key safe for SSH and cluster access.
* Ensure `kubectl` version matches your EKS cluster version for compatibility.


<br>
<br>


---
---
---


<br>
<br>


# **Argo CD on EKS â€“ Public Access Setup**

## **1. Create Namespace for Argo CD**

```bash
kubectl create namespace argocd
```

* Creates a dedicated namespace `argocd` for all Argo CD resources.

---

## **2. Install Argo CD**

```bash
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
```

* Deploys all Argo CD components in the `argocd` namespace.
* Components include: `argocd-server`, `repo-server`, `redis`, `dex-server`, metrics, etc.

---

## **3. Expose Argo CD Server via LoadBalancer**

```bash
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
```

* Changes `argocd-server` service from `ClusterIP` (internal-only) to `LoadBalancer`.
* Kubernetes provisions an **ELB (Elastic Load Balancer)** in AWS.

---

## **4. Verify Argo CD Service**

```bash
kubectl get svc -n argocd
```

Sample output:

```
NAME          TYPE           CLUSTER-IP       EXTERNAL-IP                                                                PORT(S)
argocd-server LoadBalancer   10.100.53.118    a01f8a83477ce41fba18d347280b8dcc-765224779.ap-south-1.elb.amazonaws.com   80:31888/TCP,443:32510/TCP
```

* **EXTERNAL-IP** is the public endpoint of Argo CD.
* Open this URL in a browser:

  ```
  http://a01f8a83477ce41fba18d347280b8dcc-765224779.ap-south-1.elb.amazonaws.com
  ```

---

## **5. Get Initial Admin Password**

```bash
kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
```

* This command fetches the **initial admin password** for Argo CD login.
* Username is always `admin`.

---

## âœ… **Now You Can:**

* Access Argo CD UI using the **LoadBalancer URL**.
* Login with:

  ```
  Username: admin
  Password: <output from above command>
  ```
* Start connecting Argo CD to your Git repos and deploy applications on EKS.


<br>
<br>


---
---
---


<br>
<br>


# ðŸš¨ Root cause

Kubernetes tried to **attach an EBS volume (PVC) for MongoDB**, but **the AWS EBS CSI driver or storage class** couldnâ€™t finish volume binding within timeout.

In your case, the default storage class is:

```bash
kubectl get storageclass
NAME   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION
gp2    kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false
```

Thatâ€™s an **old in-tree AWS provisioner**, not the modern **EBS CSI driver** â€” and it often causes this `VolumeBinding timeout` issue in EKS clusters that use IAM roles or private subnets.

---

### âœ… Step-by-step fix

#### **Step 1 â€” Install the AWS EBS CSI driver**

If not already installed:

```bash
eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster <your-cluster-name> --approve

eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster <your-cluster-name> \
  --role-name AmazonEKS_EBS_CSI_DriverRole \
  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  --approve

eksctl create addon \
  --name aws-ebs-csi-driver \
  --cluster <your-cluster-name> \
  --service-account-role-arn arn:aws:iam::<your-account-id>:role/AmazonEKS_EBS_CSI_DriverRole \
  --force
```

```bash
eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster three-tier-cluster --approve

eksctl create iamserviceaccount \
  --name ebs-csi-controller-sa \
  --namespace kube-system \
  --cluster three-tier-cluster \
  --role-name AmazonEKS_EBS_CSI_DriverRole \
  --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
  --approve

eksctl create addon \
  --name aws-ebs-csi-driver \
  --cluster three-tier-cluster \
  --service-account-role-arn arn:aws:iam::570248390779:role/AmazonEKS_EBS_CSI_DriverRole \
  --force
```



âœ… This adds the CSI driver EKS needs to dynamically create and attach EBS volumes.

---

#### **Step 2 â€” Create a new, modern, expandable storage class**

```bash
kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3-expandable
provisioner: ebs.csi.aws.com
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
  fsType: ext4
EOF
```

---

#### **Step 3 â€” Update your Mongo StatefulSet YAML**

Edit your manifest (`mongo-statefulset.yaml`) and set:

```yaml
volumeClaimTemplates:
- metadata:
    name: mongo-persistent-storage
  spec:
    accessModes: ["ReadWriteOnce"]
    storageClassName: gp3-expandable
    resources:
      requests:
        storage: 8Gi
```

Save and commit that.

---

#### **Step 4 â€” Delete the old StatefulSet and PVC**

(only if data loss is okay â€” since volume binding is stuck anyway)

```bash
kubectl delete statefulset mongo -n go-todo
kubectl delete pvc -n go-todo mongo-persistent-storage-mongo-0
```

Then re-sync in ArgoCD or re-apply your manifest:

```bash
kubectl apply -f mongo-statefulset.yaml
```

---

#### **Step 5 â€” Verify itâ€™s working**

Check events and pod status:

```bash
kubectl get pods -n go-todo
kubectl describe pod mongo-0 -n go-todo
kubectl get pvc -n go-todo
```

You should see:

* PVC status â†’ `Bound`
* Pod status â†’ `Running`

---

### ðŸ’¡ TL;DR

| Issue                      | Fix                                      |
| -------------------------- | ---------------------------------------- |
| EBS volume binding timeout | Install AWS EBS CSI driver               |
| Outdated `gp2` provisioner | Use `ebs.csi.aws.com`                    |
| Non-expandable storage     | Use `allowVolumeExpansion: true`         |
| PVC stuck                  | Delete old PVC + StatefulSet and reapply |

<br>
<br>


---
---
---


<br>
<br>

# MongoDB URI â€” Special Character Issue & Fix

## ðŸš¨ Problem
If your MongoDB username or password contains **special characters** (like `@`, `:`, `!`, `/`, etc.),  
the connection string breaks â€” for example:

```bash
mongodb://admin:S3cureP@ssw0rd123!@mongo-service:27017
````

MongoDB will fail to connect because `@` and other symbols confuse the URI parser.

---

## âœ… Fix â€” URL Encode the Special Characters

Replace special characters with their **percent-encoded** form.

| Character | Encoded |
| --------- | ------- |
| `@`       | `%40`   |
| `:`       | `%3A`   |
| `/`       | `%2F`   |
| `#`       | `%23`   |
| `?`       | `%3F`   |
| `%`       | `%25`   |
| `!`       | `%21`   |

### Example

If your password is:

```
S3cureP@ssw0rd123!
```

Use this in your connection string:

```bash
mongodb://admin:S3cureP%40ssw0rd123%21@mongo-service:27017
```

---

## ðŸ’¡ Best Practice

* Store credentials in **Kubernetes Secrets** or **environment variables**
* Avoid writing raw passwords directly in your YAML or code

---

âœ… Example (safe version using env vars):

```bash
mongodb://$(MONGO_ROOT_USERNAME):$(MONGO_ROOT_PASSWORD)@mongo-service:27017
```

```bash
"mongodb://$(MONGO_ROOT_USERNAME):$(MONGO_ROOT_PASSWORD)@mongo-service:27017"
mongodb://admin:S3cureP@ssw0rd123!@mongo-service:27017

"mongodb://admin:sayem@mongo-service:27017"
```


<br>
<br>


---
---
---


<br>
<br>

* **EXTERNAL-IP** is the public endpoint of Argo CD.
* Open this URL in a browser:

  ```
  http://a01f8a83477ce41fba18d347280b8dcc-765224779.ap-south-1.elb.amazonaws.com
  user-name : admin
  password : lCrA811wDXzt5img
  APP HEALTH : Healthy
  ```


- **Application Code Repo:** [EKS-GO-TODO](https://github.com/SAYEM-16T/EKS-GO-TODO.git)  
Contains backend, frontend, and base manifests.

- **Manifest Repo (GitOps):** [EKS-GITOPS](https://github.com/SAYEM-16T/EKS-GITOPS.git)  
Used for ArgoCD to manage deployments and sync with the Kubernetes cluster.


<br>
<br>

![alt text](image.png)